{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marieandrepayfit/Marie-Andr-/blob/main/TEST_de_Daily_occupancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "try:\n",
        "  import gspread\n",
        "except ModuleNotFoundError:\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    %pip install gspread\n",
        "  import gspread\n",
        "\n",
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER SERVICE LEVEL ###\n",
        "    # Summarize per Service Level\n",
        "    occupancy_summary_service_level = daily_totals.groupby(['Date day', 'Country', 'Service Level']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_service_level['Avg Working time (h)'] = occupancy_summary_service_level['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy'] = occupancy_summary_service_level['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy Adjusted'] = occupancy_summary_service_level['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg ci-co SF (h)'] = occupancy_summary_service_level['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co Intercom (h)'] = occupancy_summary_service_level['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co during lunch (h)'] = occupancy_summary_service_level['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Date day', 'Service Level', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level[columns_order]\n",
        "\n",
        "    # Sort by 'Service Level'\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.sort_values(by=['Country', 'Service Level', 'Date day'], ascending=True)\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_service_level\n",
        "\n",
        "### ... ###\n",
        "\n",
        "# Copy databases from Gsheets\n",
        "# Authentication and authorization\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "# Open spreadsheet\n",
        "worksheet = gc.open('% occupancy')\n",
        "# Replace df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(0)  # Assuming the first worksheet, adjust if needed\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "# Replace df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(1)  # Assuming the second worksheet, adjust if needed\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and get the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)# Assuming 'merged_df' is your final DataFrame\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2) # 2 decimals in the final table\n",
        "occupancy_summary_with_metrics  # Display the summary\n",
        "occupancy_summary_with_metrics['Date day'] = occupancy_summary_with_metrics['Date day'].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notnull(x) else x)\n",
        "\n",
        "# Copy final table to Gsheets\n",
        "# Authentication and authorization\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "# Open spreadsheet\n",
        "worksheet = gc.open('% occupancy')\n",
        "sheet6 = worksheet.get_worksheet(5)\n",
        "occupancy_summary_with_metrics = pd.DataFrame(occupancy_summary_with_metrics)\n",
        "sheet6.clear() #clean the tab\n",
        "sheet6.update([occupancy_summary_with_metrics.columns.values.tolist()] + occupancy_summary_with_metrics.fillna(-1).values.tolist()) #copy the table"
      ],
      "metadata": {
        "id": "PDShslzixlck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marieandrepayfit/Marie-Andr-/blob/main/Automation_Weekly_occupancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import json\n",
        "import os\n",
        "\n",
        "API_KEY = os.environ.get('DRIVE_API_KEY')\n",
        "\n",
        "# Configuration for authentication using the API key\n",
        "scope = [\n",
        "    \"https://spreadsheets.google.com/feeds\",\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive.file\",\n",
        "    \"https://www.googleapis.com/auth/drive\"\n",
        "]\n",
        "\n",
        "# Load the API key\n",
        "creds_dict = json.loads(API_KEY)\n",
        "\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_sf['Week'] = pd.to_datetime(df_sf['1.1 - Events Event Date Week'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_intercom['Week'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Week'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Week'] = df_sf['1.1 - Events Event Date Week']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Week'] = df_intercom['1.1 - Events Event Date Week']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Week', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    #daily_totals = pd.merge(daily_totals, sum_duration_ci_co_lunch, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "###### TEST #######\n",
        "    # Specify the name of the person and the date for which you want to extract the list of ci / co\n",
        "    #agent_email = \"amandine.malegue@payfit.com\"\n",
        "    #date_day = \"2024-01-03\"\n",
        "    # Filter the DataFrame based on the agent and date\n",
        "    #filtered_data = merged_df[(merged_df['Agent Email'] == agent_email) & (merged_df['Date day'] == date_day)]\n",
        "    #print(\"Columns in filtered_data:\", filtered_data.columns)\n",
        "    # Display the list of ci / co for the specified date\n",
        "    #ci_co_list = filtered_data[['Clock In', 'Clock out', 'Median Duration on the last 30 days', 'Duration ci-co (s)', 'Duration ci-co Adjusted (s)']]\n",
        "    #print(ci_co_list)\n",
        "####### ... #######\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER SERVICE LEVEL ###\n",
        "    # Summarize per Service Level\n",
        "    occupancy_summary_service_level = daily_totals.groupby(['Week', 'Country', 'Service Level']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Date day': 'nunique',\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_service_level['Avg Working time (h)'] = occupancy_summary_service_level['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy'] = occupancy_summary_service_level['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy Adjusted'] = occupancy_summary_service_level['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg ci-co SF (h)'] = occupancy_summary_service_level['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co Intercom (h)'] = occupancy_summary_service_level['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co during lunch (h)'] = occupancy_summary_service_level['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Week', 'Service Level', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level[columns_order]\n",
        "\n",
        "    # Sort by 'Service Level'\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.sort_values(by=['Country', 'Service Level', 'Week'], ascending=True)\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_service_level\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Weekly_slvl'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDShslzixlck",
        "outputId": "e00d0d04-34e3-4725-ca41-5acf3ca9cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-113e93073ecd>:53: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
            "  merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
            "<ipython-input-1-113e93073ecd>:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1bwVEpVquP4LCA_3hU5xuLlajE1PbwyXJf3c3MDuGeEM',\n",
              " 'updatedRange': 'Weekly_slvl!A1:N102',\n",
              " 'updatedRows': 102,\n",
              " 'updatedColumns': 14,\n",
              " 'updatedCells': 1428}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_sf['Week'] = pd.to_datetime(df_sf['1.1 - Events Event Date Week'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_intercom['Week'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Week'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Week'] = df_sf['1.1 - Events Event Date Week']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Week'] = df_intercom['1.1 - Events Event Date Week']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Week', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    #daily_totals = pd.merge(daily_totals, sum_duration_ci_co_lunch, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER IC ###\n",
        "    # Summarize per IC\n",
        "    occupancy_summary_IC = daily_totals.groupby(['Week', 'Country','Service Level', 'Agent Email']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Date day': 'nunique',\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_IC['Avg Working time (h)'] = occupancy_summary_IC['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_IC['% Occupancy'] = occupancy_summary_IC['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_IC['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_IC['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_IC['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg Working time Adjusted (h)'] = occupancy_summary_IC['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_IC['% Occupancy Adjusted'] = occupancy_summary_IC['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_IC['Avg ci-co SF (h)'] = occupancy_summary_IC['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg ci-co Intercom (h)'] = occupancy_summary_IC['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg ci-co during lunch (h)'] = occupancy_summary_IC['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Week', 'Service Level', 'Agent Email', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_IC = occupancy_summary_IC[columns_order]\n",
        "\n",
        "    # Sort by 'Country', 'Service Level', 'Week'\n",
        "    occupancy_summary_IC = occupancy_summary_IC.sort_values(by=['Country', 'Service Level', 'Week'], ascending=True)\n",
        "    occupancy_summary_IC = occupancy_summary_IC.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_IC\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Weekly_IC'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VYf6Qx61O9U",
        "outputId": "c06f15a6-e367-4919-c343-6da4f8506ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c136f57aa831>:41: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
            "  merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
            "<ipython-input-2-c136f57aa831>:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1bwVEpVquP4LCA_3hU5xuLlajE1PbwyXJf3c3MDuGeEM',\n",
              " 'updatedRange': 'Weekly_IC!A1:O1075',\n",
              " 'updatedRows': 1075,\n",
              " 'updatedColumns': 15,\n",
              " 'updatedCells': 16125}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_sf['Week'] = pd.to_datetime(df_sf['1.1 - Events Event Date Week'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_intercom['Week'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Week'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Week'] = df_sf['1.1 - Events Event Date Week']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Week'] = df_intercom['1.1 - Events Event Date Week']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Week', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    #daily_totals = pd.merge(daily_totals, sum_duration_ci_co_lunch, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER COUNTRY ###\n",
        "    # Summarize per country\n",
        "    occupancy_summary_country = daily_totals.groupby(['Week', 'Country']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Date day': 'nunique',\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_country['Avg Working time (h)'] = occupancy_summary_country['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_country['% Occupancy'] = occupancy_summary_country['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_country['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_country['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_country['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_country['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_country['Avg Working time Adjusted (h)'] = occupancy_summary_country['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_country['% Occupancy Adjusted'] = occupancy_summary_country['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_country['Avg ci-co SF (h)'] = occupancy_summary_country['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_country['Avg ci-co Intercom (h)'] = occupancy_summary_country['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_country['Avg ci-co during lunch (h)'] = occupancy_summary_country['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Week', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_country = occupancy_summary_country[columns_order]\n",
        "\n",
        "    # Sort by 'Country', 'Service Level', 'Week'\n",
        "    occupancy_summary_country = occupancy_summary_country.sort_values(by=['Country', 'Week'], ascending=True)\n",
        "    occupancy_summary_country = occupancy_summary_country.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_country\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Weekly_country'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMm2wx-M_Pq5",
        "outputId": "94907aa8-be43-495b-d778-464669e5ae6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e437ad11816d>:41: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
            "  merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
            "<ipython-input-3-e437ad11816d>:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1bwVEpVquP4LCA_3hU5xuLlajE1PbwyXJf3c3MDuGeEM',\n",
              " 'updatedRange': 'Weekly_country!A1:M16',\n",
              " 'updatedRows': 16,\n",
              " 'updatedColumns': 13,\n",
              " 'updatedCells': 208}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    Results / month / service level\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_sf['Week'] = pd.to_datetime(df_sf['1.1 - Events Event Date Week'], errors='coerce')\n",
        "    df_sf['Month'] = pd.to_datetime(df_sf['1.1 - Events Event Date Month'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_intercom['Week'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Week'], errors='coerce')\n",
        "    df_intercom['Month'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Month'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Month'] = df_sf['1.1 - Events Event Date Month']\n",
        "    df_sf['Week'] = df_sf['1.1 - Events Event Date Week']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Month'] = df_intercom['1.1 - Events Event Date Month']\n",
        "    df_intercom['Week'] = df_intercom['1.1 - Events Event Date Week']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Month', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Month', 'Week', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER SERVICE LEVEL ###\n",
        "    # Summarize per Service Level\n",
        "    occupancy_summary_service_level = daily_totals.groupby(['Month', 'Country', 'Service Level']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Date day': 'nunique',\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_service_level['Avg Working time (h)'] = occupancy_summary_service_level['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy'] = occupancy_summary_service_level['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy Adjusted'] = occupancy_summary_service_level['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg ci-co SF (h)'] = occupancy_summary_service_level['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co Intercom (h)'] = occupancy_summary_service_level['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co during lunch (h)'] = occupancy_summary_service_level['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Month', 'Service Level', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level[columns_order]\n",
        "\n",
        "    # Sort by 'Service Level'\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.sort_values(by=['Country', 'Service Level', 'Month'], ascending=True)\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_service_level\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Monthly_slvl'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "id": "03ZLH6GSK-p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    Results / month / country\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_sf['Month'] = pd.to_datetime(df_sf['1.1 - Events Event Date Month'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    df_intercom['Month'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Month'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Month'] = df_sf['1.1 - Events Event Date Month']\n",
        "    df_sf['Week'] = df_sf['1.1 - Events Event Date Week']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Month'] = df_intercom['1.1 - Events Event Date Month']\n",
        "    df_intercom['Week'] = df_intercom['1.1 - Events Event Date Week']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Month', 'Week', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Month', 'Week', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER COUNTRY ###\n",
        "    # Summarize per Country\n",
        "    occupancy_summary_service_level = daily_totals.groupby(['Month', 'Country']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Date day': 'nunique',\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_service_level['Avg Working time (h)'] = occupancy_summary_service_level['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy'] = occupancy_summary_service_level['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy Adjusted'] = occupancy_summary_service_level['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg ci-co SF (h)'] = occupancy_summary_service_level['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co Intercom (h)'] = occupancy_summary_service_level['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co during lunch (h)'] = occupancy_summary_service_level['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Country'\n",
        "    columns_order = ['Country', 'Month', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level[columns_order]\n",
        "\n",
        "    # Sort by 'Country'\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.sort_values(by=['Country', 'Month'], ascending=True)\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_service_level\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Monthly_country'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "id": "OhbMO6FpLm7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
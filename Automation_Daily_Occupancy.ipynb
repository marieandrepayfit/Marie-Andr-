{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marieandrepayfit/Marie-Andr-/blob/main/Automation_Daily_Occupancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import json\n",
        "import os\n",
        "\n",
        "API_KEY = os.environ.get('DRIVE_API_KEY')\n",
        "\n",
        "# Configuration for authentication using the API key\n",
        "scope = [\n",
        "    \"https://spreadsheets.google.com/feeds\",\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive.file\",\n",
        "    \"https://www.googleapis.com/auth/drive\"\n",
        "]\n",
        "\n",
        "# Load the API key\n",
        "creds_dict = json.loads(API_KEY)\n",
        "\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                               np.where((merged_df['Service Level'] == 'Resolution - Absences') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                        np.where((merged_df['Service Level'] == 'Resolution - App & Donnees') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                 np.where((merged_df['Service Level'] == 'Resolution - Contrats') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                          np.where((merged_df['Service Level'] == 'Resolution - DSN Event') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                   np.where((merged_df['Service Level'] == 'Resolution - Encadrement') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                            np.where((merged_df['Service Level'] == 'Resolution - Mutuelle Prevoyance') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                     np.where((merged_df['Service Level'] == 'Resolution - Onboarding') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                              np.where((merged_df['Service Level'] == 'Resolution - Remuneration') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                                       np.where((merged_df['Service Level'] == 'Resolution - URSSAF DGFIP') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                                                np.where((merged_df['Service Level'] == '\tResolution HUB') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                                                         np.where((merged_df['Service Level'] == 'Relationship') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                                                                                                                                  np.where((merged_df['Service Level'] == 'Expertise - Declaration') & (merged_df['Duration ci-co (s)'] > 12000), 1, #3,3h\n",
        "                                                                                                                                                                                                                                                                                                           np.where((merged_df['Service Level'] == 'Expertise - Payroll') & (merged_df['Duration ci-co (s)'] > 18000), 1, 0))))))))))))))))))))))))))))))) #5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER SERVICE LEVEL ###\n",
        "    # Summarize per Service Level\n",
        "    occupancy_summary_service_level = daily_totals.groupby(['Date day', 'Country', 'Service Level']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_service_level['Avg Working time (h)'] = occupancy_summary_service_level['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy'] = occupancy_summary_service_level['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg Working time Adjusted (h)'] = occupancy_summary_service_level['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_service_level['% Occupancy Adjusted'] = occupancy_summary_service_level['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_service_level['Avg ci-co SF (h)'] = occupancy_summary_service_level['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co Intercom (h)'] = occupancy_summary_service_level['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_service_level['Avg ci-co during lunch (h)'] = occupancy_summary_service_level['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Date day', 'Service Level', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level[columns_order]\n",
        "\n",
        "    # Sort by 'Service Level'\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.sort_values(by=['Country', 'Service Level', 'Date day'], ascending=True)\n",
        "    occupancy_summary_service_level = occupancy_summary_service_level.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_service_level\n",
        "\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Daily_slvl'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "PDShslzixlck",
        "outputId": "e6308ec2-06a4-4393-fa1d-2210b012dedb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-815b3d4c069c>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcreds_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServiceAccountCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_keyfile_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreds_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    340\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    341\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    #print(df_intercom.dtypes)\n",
        "    #print(df_sf.dtypes)\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    #daily_totals = pd.merge(daily_totals, sum_duration_ci_co_lunch, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER COUNTRY ###\n",
        "    # Summarize per country\n",
        "    occupancy_summary_country = daily_totals.groupby(['Date day', 'Country']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_country = occupancy_summary_country.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_country['Avg Working time (h)'] = occupancy_summary_country['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_country['% Occupancy'] = occupancy_summary_country['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_country['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_country['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_country['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_country['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_country['Avg Working time Adjusted (h)'] = occupancy_summary_country['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_country['% Occupancy Adjusted'] = occupancy_summary_country['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_country['Avg ci-co SF (h)'] = occupancy_summary_country['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_country['Avg ci-co Intercom (h)'] = occupancy_summary_country['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_country['Avg ci-co during lunch (h)'] = occupancy_summary_country['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Date day', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_country = occupancy_summary_country[columns_order]\n",
        "\n",
        "    # Sort by 'Country', 'Service Level', 'Date day'\n",
        "    occupancy_summary_country = occupancy_summary_country.sort_values(by=['Country', 'Date day'], ascending=True)\n",
        "    occupancy_summary_country = occupancy_summary_country.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_country\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Daily_country'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "id": "yd7vcwjkRKqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom, daily_working_hours=7.8):\n",
        "    \"\"\"\n",
        "    df_sf_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=X8P3JQXodONwAIGLIKuUeR&origin_space=2180&toggle=fil\n",
        "    df_intercom_V3 : https://payfit.eu.looker.com/explore/customer_success/cs_metrics?qid=tFJthLmYABynCLSIh2FQ7d&origin_space=2180&toggle=fil\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert event datetime to pandas datetime\n",
        "    df_sf['Clock out'] = pd.to_datetime(df_sf['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_sf['Date day'] = pd.to_datetime(df_sf['1.1 - Events Event Date Date'], errors='coerce')\n",
        "    #-#\n",
        "    df_intercom['Clock out'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Second'], errors='coerce')\n",
        "    df_intercom['Date day'] = pd.to_datetime(df_intercom['1.1 - Events Event Date Date'], errors='coerce')\n",
        "\n",
        "    # Define columns name\n",
        "    df_sf['Agent Email'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Payfiter e-mail']\n",
        "    df_sf['Service Level'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Service Level']\n",
        "    df_sf['Case ID'] = df_sf['1.2 - Cases Case ID']\n",
        "    df_sf['Date day'] = df_sf['1.1 - Events Event Date Date']\n",
        "    df_sf['Duration ci-co (s)'] = pd.to_numeric(df_sf['1.1 - Events Effective Time Spent Salesforce'], errors='coerce')\n",
        "    df_sf['Country'] = df_sf['2.2 - Payfiter - Event Modifier - Dynamic Scope country code']\n",
        "    df_sf['Duration SF (s)'] = df_sf['Duration ci-co (s)']\n",
        "    df_sf['Duration Intercom (s)'] = 0\n",
        "    #-#\n",
        "    df_intercom['Agent Email'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Payfiter e-mail']\n",
        "    df_intercom['Service Level'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Service Level']\n",
        "    df_intercom['Case ID'] = df_intercom['1.2 - Cases Case ID']\n",
        "    df_intercom['Date day'] = df_intercom['1.1 - Events Event Date Date']\n",
        "    df_intercom['Duration ci-co (s)'] = pd.to_numeric(df_intercom['1.1 - Events Effective Time Spent Intercom'], errors='coerce')\n",
        "    df_intercom['Country'] = df_intercom['2.1 - Payfiter - Event Owner - Dynamic Scope country code']\n",
        "    df_intercom['Duration SF (s)'] = 0\n",
        "    df_intercom['Duration Intercom (s)'] = df_intercom['Duration ci-co (s)']\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    merged_df = pd.merge(df_sf, df_intercom, on=['Agent Email', 'Service Level', 'Case ID', 'Date day', 'Duration ci-co (s)', 'Country', 'Clock out', 'Duration SF (s)', 'Duration Intercom (s)'], how='outer', indicator=True)\n",
        "    #print(merged_df.columns)\n",
        "\n",
        "    # Add measure for counting clock-outs at 8pm\n",
        "    merged_df['Clock Out Hour'] = merged_df['Clock out'].dt.hour\n",
        "    merged_df['Clock Out Minute'] = merged_df['Clock out'].dt.minute\n",
        "    merged_df['Clock Out at 20:00?'] = ((merged_df['Clock Out Hour'] == 20) & (merged_df['Clock Out Minute'] == 00))\n",
        "    # Add measure for counting ci-co during lunch\n",
        "    merged_df['Clock In'] = merged_df['Clock out'] - pd.to_timedelta(merged_df['Duration ci-co (s)'], unit='s')\n",
        "    merged_df['Clock In Hour'] = merged_df['Clock In'].dt.hour\n",
        "    merged_df['Clock In Minute'] = merged_df['Clock In'].dt.minute\n",
        "    merged_df['Clock In/Out lunch?'] = ((merged_df['Clock In Hour'] >= 11) & (merged_df['Clock In Hour'] <= 12) & (merged_df['Clock In Minute'] >= 30) & (merged_df['Clock Out Hour'] >= 13) & (merged_df['Clock Out Hour'] <= 14) & (merged_df['Clock Out Minute'] >= 30))\n",
        "\n",
        "    # Add a new column for the duration during lunch\n",
        "    merged_df['Duration during Lunch (s)'] = 0\n",
        "    # Filter rows where 'Clock In/Out lunch?' is True\n",
        "    lunch_filter = merged_df['Clock In/Out lunch?']\n",
        "    # Calculate the duration during lunch for rows where 'Clock In/Out lunch?' is True\n",
        "    merged_df.loc[lunch_filter, 'Duration during Lunch (s)'] = merged_df.loc[lunch_filter, 'Duration ci-co (s)']\n",
        "\n",
        "    # Exclude rows where the date of 'Clock In' is different from the date of 'Clock Out'\n",
        "    merged_df = merged_df[merged_df['Clock In'].dt.date == merged_df['Clock out'].dt.date]\n",
        "\n",
        "    # Flag aberrant values based on service level\n",
        "    merged_df['Aberrant Duration'] = np.where((merged_df['Service Level'] == 'CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                      np.where((merged_df['Service Level'] == 'APS') & (merged_df['Duration ci-co (s)'] > 18000), 1, #5h\n",
        "                                               np.where((merged_df['Service Level'] == 'OBS') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                        np.where((merged_df['Service Level'] == 'CSM - Low touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                 np.where((merged_df['Service Level'] == 'CSM - Medium touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                          np.where((merged_df['Service Level'] == 'CSM - High touch') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                   np.where((merged_df['Service Level'] == 'Decla - DSN évènementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                            np.where((merged_df['Service Level'] == 'Declaration - DSN mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                     np.where((merged_df['Service Level'] == 'Decla - Investigation') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                              np.where((merged_df['Service Level'] == 'Decla - Paramétrage') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                       np.where((merged_df['Service Level'] == 'CSM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                np.where((merged_df['Service Level'] == 'CCM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                         np.where((merged_df['Service Level'] == 'Ext CCR') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                  np.where((merged_df['Service Level'] == 'Ext CSM/AM') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                           np.where((merged_df['Service Level'] == 'Ext Evenementielles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                    np.where((merged_df['Service Level'] == 'Ext Mensuelles') & (merged_df['Duration ci-co (s)'] > 12600), 1, #3,5h\n",
        "                                                                                                                                                                             np.where((merged_df['Service Level'] == 'Ext Paramétrages') & (merged_df['Duration ci-co (s)'] > 9000), 1, #2,5h\n",
        "                                                                                                                                                                                      np.where((merged_df['Service Level'] == 'Ext OB') & (merged_df['Duration ci-co (s)'] > 9000), 1, 0)))))))))))))))))) #2,5h\n",
        "\n",
        "\n",
        "    # Calculation Moving Medians (last 30 days)\n",
        "    # Convert 'Date day' in merged_df to datetime and sort\n",
        "    merged_df['Date day'] = pd.to_datetime(merged_df['Date day'], errors='coerce')\n",
        "    merged_df.sort_values(by=['Clock out', 'Agent Email'], inplace=True)\n",
        "    # Filter merged_df to calculate the median without clock out auto and aberrant duration\n",
        "    filtered_df = merged_df[(merged_df['Clock Out at 20:00?'] == False) &\n",
        "                            (merged_df['Aberrant Duration'] == False) &\n",
        "                            (merged_df['Duration ci-co (s)'] != 0)]\n",
        "    # Calculate the moving median per IC based on the last 30 days\n",
        "    filtered_df.loc[:, 'Median Duration on the last 30 days'] = filtered_df.groupby(['Agent Email'])['Duration ci-co (s)'].transform(lambda x: x.rolling(window=30, min_periods=1).median())\n",
        "    # Merge the DataFrames\n",
        "    merged_df = pd.merge(merged_df, filtered_df[['Agent Email', 'Date day', 'Clock out', 'Median Duration on the last 30 days']], how='left')\n",
        "    # Replace NaN values (when clock out auto or aberrant duration) with the previous median of the same Date day and Agent Email\n",
        "    merged_df.sort_values(by=['Clock out', 'Date day', 'Agent Email'], inplace=True)\n",
        "    merged_df['Median Duration on the last 30 days'] = merged_df.groupby(['Agent Email', 'Date day'])['Median Duration on the last 30 days'].fillna(method='ffill')\n",
        "\n",
        "    # Calculate daily totals per IC\n",
        "    daily_totals = merged_df.groupby(['Country', 'Service Level', 'Agent Email', 'Date day']).agg({\n",
        "        'Duration ci-co (s)': 'sum',\n",
        "        'Clock Out at 20:00?' : 'sum',\n",
        "        'Clock In/Out lunch?' : 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'sum',\n",
        "        'Duration Intercom (s)' :'sum',\n",
        "        'Median Duration on the last 30 days' : 'sum',\n",
        "        'Duration during Lunch (s)' : 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Aberrant Duration' is True\n",
        "    merged_df['Duration ci-co Adjusted aberrant (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'],axis=1)\n",
        "    # Replace 'Duration ci-co (s)' with median when 'Clock Out at 20:00?' is True\n",
        "    merged_df['Duration ci-co Adjusted co 20:00 (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Clock Out at 20:00?'] and row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "    # Combine both adjustments in a single metric\n",
        "    merged_df['Duration ci-co Adjusted (s)'] = merged_df.apply(lambda row: row['Median Duration on the last 30 days'] if (row['Aberrant Duration'] or row['Clock Out at 20:00?']) and (row['Median Duration on the last 30 days'] < row['Duration ci-co (s)']) else row['Duration ci-co (s)'], axis=1)\n",
        "\n",
        "    # Add the calculation of the sum of Durations per day and per IC\n",
        "    sum_duration_aberrant_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted aberrant (s)'].sum().reset_index()\n",
        "    sum_duration_co20_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted co 20:00 (s)'].sum().reset_index()\n",
        "    sum_duration_adjusted_per_day_ic = merged_df.groupby(['Date day', 'Agent Email'])['Duration ci-co Adjusted (s)'].sum().reset_index()\n",
        "\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_aberrant_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_co20_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    daily_totals = pd.merge(daily_totals, sum_duration_adjusted_per_day_ic, on=['Date day', 'Agent Email'], how='left')\n",
        "    #daily_totals = pd.merge(daily_totals, sum_duration_ci_co_lunch, on=['Date day', 'Agent Email'], how='left')\n",
        "\n",
        "    # Convert 'Duration ci-co' to numeric\n",
        "    daily_totals['Duration ci-co'] = pd.to_numeric(daily_totals['Duration ci-co (s)'], errors='coerce')\n",
        "\n",
        "    # Metrics\n",
        "    daily_totals['# Treated cases'] = daily_totals['Case ID'].apply(lambda x: len(set(x)))\n",
        "\n",
        "### PER IC ###\n",
        "    # Summarize per IC\n",
        "    occupancy_summary_IC = daily_totals.groupby(['Date day', 'Country','Service Level', 'Agent Email']).agg({\n",
        "        '# Treated cases': 'sum',\n",
        "        'Case ID': lambda x: x.tolist(),\n",
        "        'Clock Out at 20:00?': 'sum',\n",
        "        'Clock In/Out lunch?': 'sum',\n",
        "        'Duration ci-co (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted aberrant (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted co 20:00 (s)' : 'mean',\n",
        "        'Duration ci-co Adjusted (s)' : 'mean',\n",
        "        'Aberrant Duration' : 'sum',\n",
        "        'Duration SF (s)' : 'mean',\n",
        "        'Duration Intercom (s)' : 'mean',\n",
        "        'Duration during Lunch (s)' : 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename metrics if needed\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Clock Out at 20:00?': '# Clock Out at 20:00'})\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Clock In/Out lunch?': '# Clock In/Out lunch'})\n",
        "    occupancy_summary_IC = occupancy_summary_IC.rename(columns={'Aberrant Duration': '# Aberrant Duration'})\n",
        "\n",
        "    # Metrics\n",
        "    occupancy_summary_IC['Avg Working time (h)'] = occupancy_summary_IC['Duration ci-co (s)'] / 3600\n",
        "    occupancy_summary_IC['% Occupancy'] = occupancy_summary_IC['Avg Working time (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_IC['Avg Working time Adjusted aberrant (h)'] = occupancy_summary_IC['Duration ci-co Adjusted aberrant (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg Working time Adjusted co 20:00 (h)'] = occupancy_summary_IC['Duration ci-co Adjusted co 20:00 (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg Working time Adjusted (h)'] = occupancy_summary_IC['Duration ci-co Adjusted (s)'] / 3600\n",
        "    occupancy_summary_IC['% Occupancy Adjusted'] = occupancy_summary_IC['Avg Working time Adjusted (h)'] / daily_working_hours * 100\n",
        "    occupancy_summary_IC['Avg ci-co SF (h)'] = occupancy_summary_IC['Duration SF (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg ci-co Intercom (h)'] = occupancy_summary_IC['Duration Intercom (s)'] / 3600\n",
        "    occupancy_summary_IC['Avg ci-co during lunch (h)'] = occupancy_summary_IC['Duration during Lunch (s)'] / 3600\n",
        "\n",
        "    # Reorder the columns to the specified order and sort by 'Service Level'\n",
        "    columns_order = ['Country', 'Date day', 'Service Level', 'Agent Email', '# Treated cases', '# Aberrant Duration', '# Clock Out at 20:00', '# Clock In/Out lunch', 'Avg ci-co SF (h)', 'Avg ci-co Intercom (h)', 'Avg ci-co during lunch (h)', 'Avg Working time (h)', 'Avg Working time Adjusted (h)', '% Occupancy', '% Occupancy Adjusted']\n",
        "    occupancy_summary_IC = occupancy_summary_IC[columns_order]\n",
        "\n",
        "    # Sort by 'Country', 'Service Level', 'Date day'\n",
        "    occupancy_summary_IC = occupancy_summary_IC.sort_values(by=['Country', 'Service Level', 'Date day'], ascending=True)\n",
        "    occupancy_summary_IC = occupancy_summary_IC.set_index('% Occupancy Adjusted', drop=False)\n",
        "    return occupancy_summary_IC\n",
        "### ... ###\n",
        "\n",
        "spreadsheet_name = '% occupancy'\n",
        "worksheet_title = 'Daily_IC'\n",
        "worksheet_index_sf = 0  # l'index de la feuille pour df_sf\n",
        "worksheet_index_intercom = 1  # l'index de la feuille pour df_intercom\n",
        "\n",
        "# Open the spreadsheet\n",
        "worksheet = gc.open(spreadsheet_name)\n",
        "\n",
        "# Load data for df_sf\n",
        "worksheet_sf = worksheet.get_worksheet(worksheet_index_sf)\n",
        "data_sf = worksheet_sf.get_all_values()\n",
        "df_sf = pd.DataFrame(data_sf[1:], columns=data_sf[0])\n",
        "\n",
        "# Load data for df_intercom\n",
        "worksheet_intercom = worksheet.get_worksheet(worksheet_index_intercom)\n",
        "data_intercom = worksheet_intercom.get_all_values()\n",
        "df_intercom = pd.DataFrame(data_intercom[1:], columns=data_intercom[0])\n",
        "\n",
        "# Apply the function and obtain the summary\n",
        "occupancy_summary_with_metrics = calculate_occupancy_ranges_with_additional_metrics(df_sf, df_intercom)\n",
        "occupancy_summary_with_metrics = occupancy_summary_with_metrics.round(2)  # Arrondir à 2 décimales pour la table finale\n",
        "print(occupancy_summary_with_metrics)  # Afficher le résumé\n",
        "\n",
        "# Open the spreadsheet\n",
        "spreadsheet = gc.open(spreadsheet_name)\n",
        "\n",
        "try:\n",
        "    # Try to obtain the sheet by its title\n",
        "    worksheet = spreadsheet.worksheet(worksheet_title)\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # If the sheet does not exist, create it\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
        "\n",
        "# Convert the DataFrame into a list of lists, including headers\n",
        "values = [occupancy_summary_with_metrics.columns.tolist()] + occupancy_summary_with_metrics.astype(str).values.tolist()\n",
        "\n",
        "# Update the sheet with the data, starting with cell A1\n",
        "worksheet.update('A1', values)"
      ],
      "metadata": {
        "id": "FGvYK1f_Qi-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}